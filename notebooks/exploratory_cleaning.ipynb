{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "472c09bf",
   "metadata": {},
   "source": [
    "# Rutooro Text Cleaning Pipeline\n",
    "This notebook batches through raw Rutooro text files on Google Drive, cleans and deduplicates them, and outputs one sentence per line ready for language model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85660ded",
   "metadata": {},
   "source": [
    "## Setup Google Drive and Paths\n",
    "Mount your Google Drive and set where raw and processed data should live. Edit the paths below if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "RAW_DATA_DIR = Path('/content/drive/MyDrive/rutooro-mlm/data/raw')\n",
    "PROCESSED_DATA_DIR = Path('/content/drive/MyDrive/rutooro-mlm/data/processed')\n",
    "\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print('Raw data dir:', RAW_DATA_DIR)\n",
    "print('Processed data dir:', PROCESSED_DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2370f71",
   "metadata": {},
   "source": [
    "## Preview a Raw Text File\n",
    "Here we preview part of the first raw file to understand the formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83464321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "raw_files = sorted(RAW_DATA_DIR.glob('*.txt'))\n",
    "if not raw_files:\n",
    "    print('No raw text files found in', RAW_DATA_DIR)\n",
    "else:\n",
    "    example_file = raw_files[0]\n",
    "    print('Example file:', example_file)\n",
    "    with open(example_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text_sample = ''.join(itertools.islice(f, 20))\n",
    "    print(text_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249bb236",
   "metadata": {},
   "source": [
    "## Define Cleaning Functions\n",
    "Remove page numbers, stray numbers, extra whitespace and split sentences using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771bae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r'\\f', ' ', text)           # remove form feed characters\n",
    "    text = re.sub(r'\\n+', ' ', text)          # collapse newlines\n",
    "    text = re.sub(r'page\\s*\\d+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)      # remove stray numbers\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    try:\n",
    "        sentences = sent_tokenize(text)\n",
    "    except Exception:\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bec07",
   "metadata": {},
   "source": [
    "## Clean All Files and Collect Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "files = sorted(RAW_DATA_DIR.glob('*.txt'))\n",
    "print('Found', len(files), 'raw text files')\n",
    "for path in files:\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        raw_text = f.read()\n",
    "    cleaned = clean_text(raw_text)\n",
    "    sentences = split_sentences(cleaned)\n",
    "    sentences = [s for s in sentences if len(s.split()) >= 3]\n",
    "    all_sentences.extend(sentences)\n",
    "    print(f\"{path.name}: {len(sentences)} sentences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ec0deb",
   "metadata": {},
   "source": [
    "## Remove Duplicates and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb9691",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sentences = list(dict.fromkeys(all_sentences))\n",
    "output_path = PROCESSED_DATA_DIR / 'rutooro_sentences_clean.txt'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for s in unique_sentences:\n",
    "        f.write(s + \"\\n\")\n",
    "print('Total sentences:', len(all_sentences))\n",
    "print('Unique sentences:', len(unique_sentences))\n",
    "print('Saved to', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d944a",
   "metadata": {},
   "source": [
    "## Random Sample of Cleaned Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "sample = random.sample(unique_sentences, min(10, len(unique_sentences)))\n",
    "for s in sample:\n",
    "    print('-', s)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
